{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnSbODBRHEDt",
    "outputId": "ec168f5e-a4b3-40af-e9f9-6015daf001e2"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4sPkXtAAJS4",
    "outputId": "19901e0e-dd4b-4d62-a401-6183b54cf415"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U transformers datasets accelerate evaluate sacrebleu sentencepiece scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9hT9jWkHgs8",
    "outputId": "8f715662-bf1f-4316-c406-09e54ff60204"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import evaluate\n",
    "import joblib\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, DataCollatorWithPadding,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA:\", torch.cuda.is_available(), \"| device:\", device)\n",
    "\n",
    "# SAFE import for TrainingArguments (avoids weird shadowing)\n",
    "TrainingArguments = transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289,
     "referenced_widgets": [
      "2fd51750f6b34b749e97614af2eea29f",
      "24548286600b4d5daee2efcd41f6aed0",
      "d3bda98a8af04b7689c450f95b79f973",
      "671b8445cdfc428d966423a854e03ed8",
      "b0bf96d1d75645a696773b78f539c8de",
      "e078aff9808e434383990546758e9edd",
      "40cd5b6e67f34cc7ab75d237b25451a5",
      "a017c255812c4b798abb64c769f45447",
      "1b2c7545391a499f8a98263371b1c87b",
      "6932721c9e9e4d639d66caa0b823823e",
      "4e116baca964423f8141299df7806afd",
      "ea25b4d413c642fca8c306c235cc9313",
      "a5aafa6dbe5b4547a87ef6163559e47e",
      "ee0f00ecbc854bca98f34d21364e0e4a",
      "e1650a776ba34ef1997f258c2cec9520",
      "140e8519ff174411832a00a84547ecce",
      "b14cd45650af4fd49c678d18f75278e2",
      "d88f8b33f2fa4842baa9f5ceac7d31ae",
      "3a2ccc9c4ce24ab49153314a8a11b5e5",
      "e4fc7fca7dba4e1aa68aa34a86c328a7",
      "b9e236b3162a4da49799597ffe70b39d",
      "c2587a5d46184ddab264ede494316521",
      "b40b810afd7e458283e0c1c2dc07a34d",
      "d0e815bcf0474484a100c60e08da798c",
      "accc44f20d0040f9864d024fd472e52e",
      "7d9c3f13af6749449f372c78fc4fab78",
      "0337511d4cf14594ab2cf4ecfe6f23d9",
      "d09f9520c5d44e079d92ed29172a0b47",
      "4d77e61da8ed4755921a72ecb15c5f37",
      "630a70f02544450e9494831e9277640b",
      "64a9005acc8a4de8bc3a9ab5c262007b",
      "887317828b9c499bad5e276d2246dd0d",
      "fd597588f8b245bcb8dbc5bfaa046785"
     ]
    },
    "id": "JyuxhXJAHiAs",
    "outputId": "52b6ba21-ea30-4776-f9d1-c16a39b3ee19"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 2: Load dataset + Clean\n",
    "# =========================================\n",
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "\n",
    "remove_cols = [c for c in [\"flags\", \"category\"] if c in dataset[\"train\"].column_names]\n",
    "if remove_cols:\n",
    "    dataset = dataset.remove_columns(remove_cols)\n",
    "\n",
    "full = dataset[\"train\"]\n",
    "print(\"Columns:\", full.column_names)\n",
    "print(\"Total samples:\", len(full))\n",
    "print(\"Example:\", full[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx9-QlZxHjVt",
    "outputId": "fead456c-a3f7-490f-dec2-c456e64e83a1"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 3: Split train/val/meta_dev/eval\n",
    "# 70% train_full, 30% temp\n",
    "# temp -> 50/50 => meta_dev, eval\n",
    "# train_full -> 90/10 => train, val\n",
    "# =========================================\n",
    "split_main = full.train_test_split(test_size=0.30, seed=42)\n",
    "train_full = split_main[\"train\"]\n",
    "temp = split_main[\"test\"]\n",
    "\n",
    "split_temp = temp.train_test_split(test_size=0.5, seed=42)\n",
    "meta_dev = split_temp[\"train\"]\n",
    "eval_ds = split_temp[\"test\"]\n",
    "\n",
    "split_base = train_full.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split_base[\"train\"]\n",
    "val_ds = split_base[\"test\"]\n",
    "\n",
    "data_splits = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"val\": val_ds,\n",
    "    \"meta_dev\": meta_dev,\n",
    "    \"eval\": eval_ds\n",
    "})\n",
    "\n",
    "print(\"train:\", len(data_splits[\"train\"]))\n",
    "print(\"val:\", len(data_splits[\"val\"]))\n",
    "print(\"meta_dev:\", len(data_splits[\"meta_dev\"]))\n",
    "print(\"eval:\", len(data_splits[\"eval\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234,
     "referenced_widgets": [
      "61648260ec904a65ab82127cd9b9c9f1",
      "9a434192e95343f28853bd792d581f54",
      "02c5f6f47e1548d19a9701bcb9d6989b",
      "27e999c1eae046d186e3b3843b400946",
      "385184ce17044974a0c0b23ef5d129a6",
      "eaf22529988a418f98fc214d1f7fb791",
      "51fcbf0bb61f448e882e3b7ff2660691",
      "67d41c62d3d844e48ca5b666fcb2cb40",
      "2055c0dd4388414986610b47554a19a7",
      "deed0122bb014842a3a2a0fe8000f7c8",
      "56342a2242f04a25849161f57ac194bf",
      "9f063e51dfab444db83844c0fc7426a2",
      "e695f998d48b4a3483bb0a3b7333f793",
      "80216344e0bf407984807ab75afa9280",
      "c277cd35dece4c78a58766dbba0f6999",
      "29047c13113f45218ae508bb8e7a2b9e",
      "c90eede1accd4252a93ed5d4050bd62a",
      "dba48a759a884579a3aec40dd8e9cf8c",
      "659d1992f0ea469e8d1b4afcf4fe4bba",
      "e90a99647b414d448d1a133407ae0866",
      "62eee88f69a740c589c16884146bb4f1",
      "ab8806f855134d629ac39c95531ecd8e",
      "a9cecb3cf50e474783210d1c80d7c1af",
      "db6b34f415954fa282e583f99b7da864",
      "e67bdded84e440e7bb96fcb8651a550d",
      "944bdbee15bc42bebe30e495bf96f40a",
      "e46958064d3744a19b2f5f9f5086e9e0",
      "addec10000324a6d9a25bc63c2512481",
      "12cdac1c2d3c40b69edeff233e2312d4",
      "dd586323b5f44a77bf0ba00944925f5f",
      "19a095a5e7c6473f8f3e0573f33e278e",
      "a8004f1bbcd647559c5d61f7a060835d",
      "ad46e8990d3347cba187eeb8167f0c2b",
      "5f214c60489c4524987eb13e40f03eff",
      "3e5faec0a93746f2a7462eca8f3f7a02",
      "372e1058958d440a9927aa45c961f3e4",
      "88ded2c5a5d2473b91bb0abff2d884ce",
      "3aeeff29396d4285944aac25fafd423d",
      "37471e1690c048b5b19bfaf4a21374dd",
      "d7b60f5d047244dc82c33541666c352d",
      "58b102262e394a9eb509bc74dfe6341a",
      "b6b3be76feee45779c3e2f467d7448d6",
      "d3187ad910fa4e5484ffc0031cd7fbcb",
      "3d00f60d14f7453b92b19bbc6e728bc4"
     ]
    },
    "id": "dJ1EJLHxHmLF",
    "outputId": "599c4dbe-2593-43b5-dc2b-e3d106d8c826"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 4: Label mapping for intent (from TRAIN only)\n",
    "# =========================================\n",
    "intents = sorted(list(set(data_splits[\"train\"][\"intent\"])))\n",
    "label2id = {lbl: i for i, lbl in enumerate(intents)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "print(\"Number of intents:\", len(intents))\n",
    "print(\"label2id sample:\", list(label2id.items())[:10])\n",
    "\n",
    "# sanity check\n",
    "unseen_val = set(data_splits[\"val\"][\"intent\"]) - set(intents)\n",
    "unseen_eval = set(data_splits[\"eval\"][\"intent\"]) - set(intents)\n",
    "print(\"Unseen intents in val:\", unseen_val)\n",
    "print(\"Unseen intents in eval:\", unseen_eval)\n",
    "\n",
    "def add_label(ex):\n",
    "    ex[\"label\"] = label2id[ex[\"intent\"]]\n",
    "    return ex\n",
    "\n",
    "for split in [\"train\", \"val\", \"meta_dev\", \"eval\"]:\n",
    "    data_splits[split] = data_splits[split].map(add_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "838e1a58353a498db1bf96661d0a7f46",
      "a6d69ba2647c43659e05b8554955471b",
      "2b672333b2f3492a91060acc27f749e8",
      "537bbb578c4f4102b0da8b8ed587aa55",
      "344bbecff1d1495bb477a9d475011451",
      "e01afc8b80884d3493d84e5025b94da0",
      "d966f89dfb254d11b2ea01c97397ccbc",
      "66dea9293bb74972b1efc982a964a216",
      "65cf1154b2334b4f8fb2a4d128278c8a",
      "73286307895847df808dc8863a901e15",
      "2dfd5a18de704aa489264c203401885a",
      "879fd712b367485bb81c87ab33b1d399",
      "0fa8af718c5942fbafe65f3bc4562cb0",
      "96354114fbf145b89fcef68725d90477",
      "1c3dda59fa654b2cb107863587483c47",
      "d1d0dfcf88ac4cd5b38d78b36c2e19f9",
      "970914d10c6742dcb163c51702ae7a12",
      "48ba2bd310d647feb70d79417cc27681",
      "4d7efae43aab4ab78427cb4bbe4cf94e",
      "ab3c3acd557c455d8d2f0180798046bd",
      "b19169fa8a4d4af9a013e98c88738ad1",
      "a893adcb9b7646e7811a77d861175aa5",
      "293c19fdedf145d6b335445206203e08",
      "279b5ad517064b0baa13fdea642c390c",
      "ad3afc448d944b75adab77eb0b1147e8",
      "b97b2e531006411aae148710d7261026",
      "e8a4fd0833c74ea5912a4349cc0ecb94",
      "cd18fd942b404303a42ec358717835f6",
      "37828a500dc54aef9a06653d7f8a5c90",
      "bc5fc939ff5347d1a8688a9f5d2dd552",
      "26e5a3ba550949928fde77823f324eb8",
      "728cb62710014fa1b0aa4c4276f466ed",
      "9877ec2438004b6893cfe065fdc21b76",
      "f48237d8e71148eab4a915f9e6462429",
      "8e3fe4b042c94dd9b2e7bfcd2ffdbc00",
      "9eeb965b84084a6392b04b986ecc258f",
      "c09e00ade418417f9a9b673750c7aeb1",
      "571380c7bac64491b25396c417fd540c",
      "3f7791228afc416382b9c217d563d66a",
      "1c50d981323243e1b5b2ebc45eeadab3",
      "c9b7b271964e46f394f39b0f34430f41",
      "98273cff82a3482395e03a01180df8e0",
      "0f22ef6cab67411fa747dd8c95562864",
      "847b98f4b35d418cad6168ae14fc795c",
      "6ee333ae40914e6ea92fa02c3d4cf251",
      "8d3993a24d214d419dfb49b7030eae56",
      "4d80c7b71f45427ca04417b8e148374a",
      "fa37fb90942c4bb593478079c493735c",
      "4682f62d81104c8abeadbc571e776fc8",
      "2ac06a47a3a94aba80420b6095f9a0ca",
      "5913609c79e746a2b835beb944b91301",
      "ee4f86cfa8b743599c38addb4a55d2c4",
      "f99e9bc3d94c427a9866bb20aca35a3d",
      "67c9c536d6a348fdb918ea181fd8948f",
      "b8ea46638882493d8ec96bee4f299c11",
      "ba4eb1cbeada4ad186b150c5b59d1da9",
      "f4e98623842b4de8bb666800514eb4c4",
      "498fc076340a4d29872801a2d705c4c5",
      "ce3ddbb34c5f49a0b21c0327781c185e",
      "b6ed2cc4fbb34d3b9356e00c24eec213",
      "1498b002ca4d4405a1788e1499494645",
      "aa7fa0a2ee8e4056ab9c5a44fa190de9",
      "acff770fa4e5448a98890c7500a892cb",
      "61a5ba62d18841f09d5aef96e34ba63f",
      "a0a984bb65dd478f8cd8f656157b59b3",
      "df407c5bc0954fd6876b5bd147880901",
      "a427d5485e1e44c683f374e4c18b9ec8",
      "c8bad93b678e4878af65de1ace43e921",
      "1112172d4c03460b84d03a816ae8fa68",
      "609e5d91660344688cfe1a90afb2d75f",
      "780939c0a95a4d45894f2e73ab8520ee",
      "5091d1482a3549ad8f6cc9fc21b4874c",
      "114170858fee48d7b5502b26cc6dd2aa",
      "1463e33c54bf4c64ad42d909c1f6cd8a",
      "61a1951561b94f9c87336ba5235d9976",
      "7f1a21c81070496cacf0d296009d7e79",
      "45d65a0836794974af4ff401b7652e63",
      "1ddf1523e71e4e9c8fd737524a1a1799",
      "9ec7be2a31bb49958a032d2cca8c19ce",
      "902bea32bcfa4505b6dec107961c8d7d",
      "4cc335d00eb1438eb09416489740d9d6",
      "b76afa6b256740a3beadda59e57792be",
      "863d7b58693e48bab7b95242b4d03f94",
      "5daf120c789a4f1493e19c62eedf0694",
      "8c3b0ca0f5ce46788238e182fab6f263",
      "46f8cbf2fbad4af18cf7ac9408665b16",
      "e8fda9f3b2b74a9a8a8b27c4a37c5aa1",
      "10e306c8c09f4e17a2e800a66941e38c"
     ]
    },
    "id": "7KZNOiWsHnqU",
    "outputId": "c47d1c1a-d9f4-4276-b5eb-556d8aa243bf"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 5: Tokenize for BERT\n",
    "# =========================================\n",
    "bert_name = \"bert-base-uncased\"\n",
    "bert_tok = AutoTokenizer.from_pretrained(bert_name)\n",
    "MAX_LEN_BERT = 128\n",
    "\n",
    "def tok_bert(batch):\n",
    "    return bert_tok(batch[\"instruction\"], truncation=True, max_length=MAX_LEN_BERT)\n",
    "\n",
    "tokenized_bert = data_splits.map(tok_bert, batched=True)\n",
    "\n",
    "# remove text columns from tokenized\n",
    "text_cols = [c for c in [\"instruction\", \"intent\", \"response\"] if c in tokenized_bert[\"train\"].column_names]\n",
    "for split in [\"train\", \"val\", \"meta_dev\", \"eval\"]:\n",
    "    tokenized_bert[split] = tokenized_bert[split].remove_columns(text_cols)\n",
    "    tokenized_bert[split].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "bert_collator = DataCollatorWithPadding(tokenizer=bert_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "P68avB5-Hpol",
    "outputId": "c90e7949-8c0c-43ac-e5eb-cc72eca5186c"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 6: Train BERT Intent Classifier\n",
    "# =========================================\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_name,\n",
    "    num_labels=len(intents),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics_intent(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1m = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "bert_args = TrainingArguments(\n",
    "    output_dir=\"./bert_intent\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=2000,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=bert_args,\n",
    "    train_dataset=tokenized_bert[\"train\"],\n",
    "    eval_dataset=tokenized_bert[\"val\"],\n",
    "    tokenizer=bert_tok,\n",
    "    data_collator=bert_collator,\n",
    "    compute_metrics=compute_metrics_intent\n",
    ")\n",
    "\n",
    "bert_trainer.train()\n",
    "print(\"VAL:\", bert_trainer.evaluate(tokenized_bert[\"val\"]))\n",
    "print(\"EVAL:\", bert_trainer.evaluate(tokenized_bert[\"eval\"]))\n",
    "\n",
    "bert_best_dir = \"./bert_intent_best\"\n",
    "bert_trainer.save_model(bert_best_dir)\n",
    "bert_tok.save_pretrained(bert_best_dir)\n",
    "print(\"Saved BERT to:\", bert_best_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dV6AGwxYHyPE",
    "outputId": "94eb0725-3af6-4d15-9ad2-d3093c2c10cb"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 7: Predict intent helper (batch)\n",
    "# =========================================\n",
    "@torch.no_grad()\n",
    "def predict_intents(texts, batch_size=64):\n",
    "    bert_model.eval()\n",
    "    out = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = bert_tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN_BERT).to(device)\n",
    "        logits = bert_model(**enc).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "        out.extend([id2label[j] for j in pred_ids])\n",
    "    return out\n",
    "\n",
    "demo = \"I want to reset my password but I can't access my email\"\n",
    "print(\"Demo intent:\", predict_intents([demo])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562,
     "referenced_widgets": [
      "775d474a0b5041b880830be5c1999e23",
      "5de49f478af94fc282045878a66e01fc",
      "e61798947fc04bbd8ed35e3e3932cc08",
      "84091b119a734cc68314ee50f2e28257",
      "f2c72882494543588a33ee9533f7817f",
      "9984c012f74a4450b5b4fda583594789",
      "96880c897bff4c6484f94729add9872f",
      "ea96035455b64c81b18dfd32ac671dd9",
      "ab41feec6a224c2a8215ab9b6e396314",
      "fbdcae9d58cb4f07a412217e90913450",
      "b46c34435a794766961e705ccd89665e",
      "d508b19b24224d5482313e5617d5de32",
      "12665f98b9bc4ea3810e9955d076706d",
      "66f9ae81668f4587b6c8692f428d43f9",
      "9ec8999361e34c6e9a9798097f1916d8",
      "d48351d2771d4f818bbaf011c7a552e5",
      "4ecfafcb38ae482bade6d8e88a34ed9c",
      "91da57c86ff5489fb143060c402b750e",
      "ddbfe3f82515424891ed46e469c1eb1d",
      "dd32843b9dc54b1d8c17291d4ae2c3f3",
      "0ff4f7e7bde14c24805f90e37920fbe4",
      "490dcde2cb04488e9f73539bd13e1bd6"
     ]
    },
    "id": "qkEQlo75H1ct",
    "outputId": "6a27eb5a-a8e9-4e95-a1cd-f4cb91460b45"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 8: T5 SIMPLE training (instruction -> response)\n",
    "# =========================================\n",
    "t5_name = \"t5-small\"\n",
    "t5_tok = T5Tokenizer.from_pretrained(t5_name)\n",
    "t5_simple_model = T5ForConditionalGeneration.from_pretrained(t5_name).to(device)\n",
    "\n",
    "MAX_IN_SIMPLE = 128\n",
    "MAX_OUT_SIMPLE = 128\n",
    "\n",
    "def preprocess_t5_simple(batch):\n",
    "    inputs = batch[\"instruction\"]\n",
    "    targets = batch[\"response\"]\n",
    "\n",
    "    model_inputs = t5_tok(inputs, max_length=MAX_IN_SIMPLE, truncation=True, padding=\"max_length\")\n",
    "    labels = t5_tok(text_target=targets, max_length=MAX_OUT_SIMPLE, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[tok if tok != t5_tok.pad_token_id else -100 for tok in seq] for seq in labels]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "t5_simple_train = data_splits[\"train\"].map(preprocess_t5_simple, batched=True, remove_columns=data_splits[\"train\"].column_names)\n",
    "t5_simple_val   = data_splits[\"val\"].map(preprocess_t5_simple, batched=True, remove_columns=data_splits[\"val\"].column_names)\n",
    "t5_simple_train.set_format(type=\"torch\")\n",
    "t5_simple_val.set_format(type=\"torch\")\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_bleu(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    pred_text = t5_tok.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, t5_tok.pad_token_id)\n",
    "    label_text = t5_tok.batch_decode(labels, skip_special_tokens=True)\n",
    "    score = bleu_metric.compute(predictions=pred_text, references=[[t] for t in label_text])[\"score\"]\n",
    "    return {\"bleu\": score}\n",
    "\n",
    "simple_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_simple\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=800,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "t5_collator_simple = DataCollatorForSeq2Seq(tokenizer=t5_tok, model=t5_simple_model)\n",
    "\n",
    "simple_trainer = Seq2SeqTrainer(\n",
    "    model=t5_simple_model,\n",
    "    args=simple_args,\n",
    "    train_dataset=t5_simple_train,\n",
    "    eval_dataset=t5_simple_val,\n",
    "    tokenizer=t5_tok,\n",
    "    data_collator=t5_collator_simple,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "simple_trainer.train()\n",
    "\n",
    "t5_simple_best_dir = \"./t5_simple_best\"\n",
    "simple_trainer.save_model(t5_simple_best_dir)\n",
    "t5_tok.save_pretrained(t5_simple_best_dir)\n",
    "print(\"Saved:\", t5_simple_best_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182,
     "referenced_widgets": [
      "6f19911ba89244b1a46bad56ffe873fe",
      "5049919be5154222ab9d6cffddbe2f49",
      "9e58219ae4dc4dc9a1d2759476536f20",
      "d7c4d63bc6114a8991fac5aaa30a0b21",
      "39c45b9b57894ae59e3fe2af8ce900ce",
      "8a4e4630460c4ed79629f74c0c2a2364",
      "b46b28f7b0d047a6a7bae27d7b4005ae",
      "00aa9cb0eed5454484708b99415a12c7",
      "d3fb2e35de31455bac53cf4470584498",
      "083f6fe9a3f044c4971e897e71ae713b",
      "aef740943fde42dbb90407c6e2eb9807",
      "bb3a307da347439fb507216647357fda",
      "40be4080a62947f2bc8839a38dc05cd3",
      "ffda66fa2699488da778c6b7bef7c60c",
      "ff61217edf0a4c9d954933df0c3cfdbd",
      "bb82c276ee6c4fd19163dbe982fbab9b",
      "f2885569bdb84d7285802ca72e4ecb5e",
      "c3a8819b8ce34cf4bb4a6aa9be63560a",
      "a65574ab04b6480eb0597b35067ea2fd",
      "106d3faf3b8b4ca8b274f268e7210fa2",
      "0e32aa50f37640789fa88337fdbc29ad",
      "ebfd9c0357d545b095f404b59c56e3ad",
      "a6a5b357481048249f9e8077f4a39f82",
      "e0ab1f0471954ffb99ec88490abf2dca",
      "c814272bb94749afa7b8476e34c3fa1f",
      "5c747ec21c7d4e78bf57c0af5f9a8d95",
      "d9ddd4f9dd33469384647bb391cf1524",
      "e7035a8199784b3ebae13439a8786c21",
      "38adc89279724b4b9d5a4a4dbd046eaa",
      "305c28b69755409d8b8f80f69a8ffb13",
      "016bb9a830d040e8bccc83220b672c65",
      "0a00ada86d184443a52d742c773fe568",
      "80d3da56bd3f4a66981458f1b3c0ae7d",
      "adac29195225465ba721b1d6b29b6529",
      "06466e6af13645c8b2ff50b555b04b20",
      "c98d74bc13b741779fbea9ed3aac6b21",
      "b70c98244e284736a058972d039090ee",
      "776ab61e5dff4c418498a1ddc3533309",
      "c382c801fbc0477a944cd0d580ae0229",
      "9ae08c9c7d6e42b4ba3cadb4b8f3b494",
      "3d07f3111f1b4b208f7a6eba919ddaae",
      "85063bb5da694abe8acaba27b9edff34",
      "860f5c46379a46288b891828935b4ee5",
      "e2a1473db07d43b6899aa16214066e34"
     ]
    },
    "id": "3N-07rDfH6vc",
    "outputId": "d12dc00e-1311-43f1-c6c9-f0e554f49a07"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 9: Add intent_pred to all splits\n",
    "# =========================================\n",
    "def add_intent_pred(batch):\n",
    "    texts = batch[\"instruction\"]\n",
    "    batch[\"intent_pred\"] = predict_intents(texts, batch_size=64)\n",
    "    return batch\n",
    "\n",
    "train_i    = data_splits[\"train\"].map(add_intent_pred, batched=True, batch_size=64)\n",
    "val_i      = data_splits[\"val\"].map(add_intent_pred, batched=True, batch_size=64)\n",
    "meta_dev_i = data_splits[\"meta_dev\"].map(add_intent_pred, batched=True, batch_size=64)\n",
    "eval_i     = data_splits[\"eval\"].map(add_intent_pred, batched=True, batch_size=64)\n",
    "\n",
    "print(\"Example with intent_pred:\", meta_dev_i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334,
     "referenced_widgets": [
      "be358999b7bb4e41af3b4453b70e56ce",
      "898dab0134214aeb9cad7ccbaaf970a6",
      "53ae3c1e867f4e74af735bdded3fa2f4",
      "e435beacfbce42cca173f1422f3f943b",
      "41b403a2ae5142ccba5a1312592e17de",
      "446063d404d747bfa053666450ddfb9f",
      "0d4b056fb1bb45c3a9061bf3a045bb58",
      "e47173064da84e3e9d99bcf2f05a61fc",
      "9da0346798c240cebcfb420b93d4584d",
      "0904fdd22ed5416cbe2c0bfbb27c1d7f",
      "3f70ba0a83cf4a62afd3e8f017a2144b",
      "62c60f8cfc8b4b0a8ba0ac955869aca0",
      "0a3c4464009244deb62aa3df78f8bf97",
      "48e0fe15bada438aa2fddee2d2f47663",
      "9611462c338b4bf1b192e984e3b1e4b4",
      "6816268e9c2945fd9db171d165a334e4",
      "52e288a9d4d1415e935dd44a015328da",
      "8b16cf1dd89c449aaae149a9b32f3cd3",
      "1c3c1a1efda1488da4f5adeba76f78eb",
      "bfa3b05deabd4c07bc0501dfb0a9d108",
      "800cde25d8da468f917d7c3c142c5c8b",
      "e863dbd595a74c4a840e4290e913a89c"
     ]
    },
    "id": "MhsJv9KgH8k9",
    "outputId": "ad7ab5fa-d033-4bd2-a214-9952bacea0b3"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 10: T5 COMPLEX training (intent_pred + instruction -> response)\n",
    "# =========================================\n",
    "t5_complex_model = T5ForConditionalGeneration.from_pretrained(t5_name).to(device)\n",
    "\n",
    "MAX_IN_COMPLEX = 256\n",
    "MAX_OUT_COMPLEX = 256\n",
    "\n",
    "def preprocess_t5_complex(batch):\n",
    "    inputs = [f\"intent: {ip} | {ins}\" for ip, ins in zip(batch[\"intent_pred\"], batch[\"instruction\"])]\n",
    "    targets = batch[\"response\"]\n",
    "\n",
    "    model_inputs = t5_tok(inputs, max_length=MAX_IN_COMPLEX, truncation=True, padding=\"max_length\")\n",
    "    labels = t5_tok(text_target=targets, max_length=MAX_OUT_COMPLEX, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [[tok if tok != t5_tok.pad_token_id else -100 for tok in seq] for seq in labels]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "t5_complex_train = train_i.map(preprocess_t5_complex, batched=True, remove_columns=train_i.column_names)\n",
    "t5_complex_val   = val_i.map(preprocess_t5_complex, batched=True, remove_columns=val_i.column_names)\n",
    "t5_complex_train.set_format(type=\"torch\")\n",
    "t5_complex_val.set_format(type=\"torch\")\n",
    "\n",
    "complex_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5_complex\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=800,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "\n",
    "t5_collator_complex = DataCollatorForSeq2Seq(tokenizer=t5_tok, model=t5_complex_model)\n",
    "\n",
    "complex_trainer = Seq2SeqTrainer(\n",
    "    model=t5_complex_model,\n",
    "    args=complex_args,\n",
    "    train_dataset=t5_complex_train,\n",
    "    eval_dataset=t5_complex_val,\n",
    "    tokenizer=t5_tok,\n",
    "    data_collator=t5_collator_complex,\n",
    "    compute_metrics=compute_bleu\n",
    ")\n",
    "\n",
    "complex_trainer.train()\n",
    "\n",
    "t5_complex_best_dir = \"./t5_complex_best\"\n",
    "complex_trainer.save_model(t5_complex_best_dir)\n",
    "t5_tok.save_pretrained(t5_complex_best_dir)\n",
    "print(\"Saved:\", t5_complex_best_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rlpz2OEH-1M"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 11: Load best T5 models + generation helpers\n",
    "# =========================================\n",
    "t5_simple = T5ForConditionalGeneration.from_pretrained(t5_simple_best_dir).to(device).eval()\n",
    "t5_complex = T5ForConditionalGeneration.from_pretrained(t5_complex_best_dir).to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_simple(texts, max_new_tokens=64):\n",
    "    enc = t5_tok(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outs = t5_simple.generate(**enc, max_new_tokens=max_new_tokens)\n",
    "    return t5_tok.batch_decode(outs, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_complex(intent_preds, texts, max_new_tokens=64):\n",
    "    inputs = [f\"intent: {i} | {t}\" for i, t in zip(intent_preds, texts)]\n",
    "    enc = t5_tok(inputs, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outs = t5_complex.generate(**enc, max_new_tokens=max_new_tokens)\n",
    "    return t5_tok.batch_decode(outs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "2f77aefeead94dadb97523579d82a6ef",
      "2eaf3d36a042424ba2fd822eb921f17b",
      "a71c9dc0b6894d5a963ca29f9daa4f4a",
      "316dbc3d0a7341d889df97239df7aadc",
      "1224c1b923344e61bdc9cdab3de57698",
      "978053bd41ed4b7f93724ec18d0b0bfc",
      "9ac8e9c005f64d6bad213d259eb5657a",
      "29e450215fdf4b0f8a4cfb25376f48fb",
      "691add60649c4665a936ccf2b9db6e29",
      "6dcb5eaee7ee4d398874488ccef71afb",
      "82b03c5bae6d4166b51aee2da874128d"
     ]
    },
    "id": "W8cLhWAxIBNU",
    "outputId": "48692240-a78f-436f-aa8e-32086c5e5f46"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 12: Meta-dev labeling using per-example BLEU\n",
    "# =========================================\n",
    "def label_meta_dev(batch):\n",
    "    texts = batch[\"instruction\"]\n",
    "    refs = batch[\"response\"]\n",
    "    intents_pred = batch[\"intent_pred\"]\n",
    "\n",
    "    ps = generate_simple(texts)\n",
    "    pc = generate_complex(intents_pred, texts)\n",
    "\n",
    "    labels = []\n",
    "    for s, c, r in zip(ps, pc, refs):\n",
    "        bs = bleu_metric.compute(predictions=[s], references=[[r]])[\"score\"]\n",
    "        bc = bleu_metric.compute(predictions=[c], references=[[r]])[\"score\"]\n",
    "        labels.append(\"complex\" if bc > bs else \"simple\")\n",
    "\n",
    "    batch[\"route_label\"] = labels\n",
    "    return batch\n",
    "\n",
    "meta_labeled = meta_dev_i.map(label_meta_dev, batched=True, batch_size=8)\n",
    "\n",
    "from collections import Counter\n",
    "print(\"Route label distribution:\", Counter(meta_labeled[\"route_label\"]))\n",
    "print(\"Example:\", meta_labeled[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379,
     "referenced_widgets": [
      "31006eaddba9436d91b4ae236627d249",
      "71f29be7fce64a9489c01d9316412992",
      "18915c4ab34e4c349afc8f5e09378cec",
      "f4147f01dfcc447799be57f64651c765",
      "ee0506fbbe0741fb87176eab53411731",
      "62b160f5d1fc476a9a72c4c6a7c8bdeb",
      "140df11203e8498aa27278465a786092",
      "6e543bfcb739476fbd3bbe72eb90ee21",
      "70b4da9447814e128fef659d8567b1fe",
      "a0b3a3cd5d544b74b638ff3efa575be8",
      "f9efba873bec4655b637f6e998eb3d43"
     ]
    },
    "id": "O-IimeNcICgN",
    "outputId": "1a581a72-beea-4d20-d290-270130219e36"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 13: Router features from BERT uncertainty\n",
    "# =========================================\n",
    "@torch.no_grad()\n",
    "def add_router_features(batch):\n",
    "    texts = batch[\"instruction\"]\n",
    "    enc = bert_tok(texts, padding=True, truncation=True, max_length=MAX_LEN_BERT, return_tensors=\"pt\").to(device)\n",
    "    logits = bert_model(**enc).logits\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "    conf = probs.max(axis=1)\n",
    "    top2 = np.sort(probs, axis=1)[:, -2:]\n",
    "    margin = top2[:, 1] - top2[:, 0]\n",
    "    entropy = -(probs * np.log(probs + 1e-12)).sum(axis=1)\n",
    "    lengths = [len(bert_tok(t)[\"input_ids\"]) for t in texts]\n",
    "\n",
    "    batch[\"feat_conf\"] = conf.tolist()\n",
    "    batch[\"feat_margin\"] = margin.tolist()\n",
    "    batch[\"feat_entropy\"] = entropy.tolist()\n",
    "    batch[\"feat_len\"] = lengths\n",
    "    batch[\"y\"] = [1 if lbl == \"complex\" else 0 for lbl in batch[\"route_label\"]]\n",
    "    return batch\n",
    "\n",
    "meta_feat = meta_labeled.map(add_router_features, batched=True, batch_size=64)\n",
    "\n",
    "X = np.column_stack([\n",
    "    meta_feat[\"feat_conf\"],\n",
    "    meta_feat[\"feat_margin\"],\n",
    "    meta_feat[\"feat_entropy\"],\n",
    "    meta_feat[\"feat_len\"]\n",
    "])\n",
    "y = np.array(meta_feat[\"y\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "router = LogisticRegression(max_iter=2000)\n",
    "router.fit(X_train, y_train)\n",
    "\n",
    "pred = router.predict(X_test)\n",
    "print(classification_report(y_test, pred, target_names=[\"simple\",\"complex\"]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "\n",
    "joblib.dump(router, \"./router_lr.joblib\")\n",
    "print(\"Saved: ./router_lr.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ontfQW6PIELt",
    "outputId": "a0b47b8b-6d98-4cec-8b53-737a53874fd2"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 14: Threshold tuning + save/load\n",
    "# =========================================\n",
    "probs = router.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0.4, 0.7, 301)\n",
    "best_t = None\n",
    "best_score = -1\n",
    "\n",
    "for t in thresholds:\n",
    "    pred_t = (probs > t).astype(int)\n",
    "    rate = pred_t.mean()\n",
    "    if not (0.05 < rate < 0.95):\n",
    "        continue\n",
    "    score = f1_score(y_test, pred_t, average=\"macro\")\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_t = float(t)\n",
    "\n",
    "THRESHOLD = float(best_t)\n",
    "print(\"Best threshold:\", round(THRESHOLD, 3))\n",
    "print(\"Best macro-F1:\", round(best_score, 3))\n",
    "print(\"Complex rate:\", round(((probs > THRESHOLD).astype(int)).mean(), 3))\n",
    "\n",
    "with open(\"./router_threshold.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": THRESHOLD}, f)\n",
    "print(\"Saved threshold to ./router_threshold.json\")\n",
    "\n",
    "with open(\"./router_threshold.json\") as f:\n",
    "    THRESHOLD = json.load(f)[\"threshold\"]\n",
    "print(\"Using THRESHOLD =\", THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o13Buj-yiMWn",
    "outputId": "513b5df2-44b7-418b-8569-594e82e18976"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_best = (probs > THRESHOLD).astype(int)\n",
    "print(classification_report(y_test, pred_best, target_names=[\"simple\",\"complex\"]))\n",
    "print(confusion_matrix(y_test, pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H93SZic3IGnc",
    "outputId": "191f36fe-dc7b-4112-8df8-fccfcf362d74"
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CELL 15: Apply router to EVAL + demo outputs\n",
    "# =========================================\n",
    "@torch.no_grad()\n",
    "def build_router_features(texts):\n",
    "    enc = bert_tok(texts, padding=True, truncation=True, max_length=MAX_LEN_BERT, return_tensors=\"pt\").to(device)\n",
    "    logits = bert_model(**enc).logits\n",
    "    probs_ = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "    conf_ = probs_.max(axis=1)\n",
    "    top2_ = np.sort(probs_, axis=1)[:, -2:]\n",
    "    margin_ = top2_[:, 1] - top2_[:, 0]\n",
    "    entropy_ = -(probs_ * np.log(probs_ + 1e-12)).sum(axis=1)\n",
    "    len_ = np.array([len(bert_tok(t)[\"input_ids\"]) for t in texts])\n",
    "    return np.column_stack([conf_, margin_, entropy_, len_])\n",
    "\n",
    "eval_texts = list(eval_i[\"instruction\"])\n",
    "\n",
    "X_eval = build_router_features(eval_texts)\n",
    "\n",
    "probs_eval = router.predict_proba(X_eval)[:, 1]\n",
    "route_eval = (probs_eval > THRESHOLD).astype(int)  # 1=complex, 0=simple\n",
    "print(\"Complex rate on EVAL:\", route_eval.mean())\n",
    "\n",
    "# show a few examples\n",
    "idxs = list(range(5))\n",
    "sample_texts = [eval_texts[i] for i in idxs]\n",
    "sample_intents = [eval_i[\"intent_pred\"][i] for i in idxs]\n",
    "\n",
    "pred_s = generate_simple(sample_texts)\n",
    "pred_c = generate_complex(sample_intents, sample_texts)\n",
    "\n",
    "for k, i in enumerate(idxs):\n",
    "    r = \"complex\" if route_eval[i] == 1 else \"simple\"\n",
    "    print(\"\\n---\")\n",
    "    print(\"Q:\", sample_texts[k])\n",
    "    print(\"intent_pred:\", sample_intents[k])\n",
    "    print(\"route:\", r)\n",
    "    print(\"simple:\", pred_s[k])\n",
    "    print(\"complex:\", pred_c[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-CJrm54kPO5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def safe_text_list(x):\n",
    "    # converts HF datasets column / np arrays to pure python list[str]\n",
    "    if x is None:\n",
    "        return []\n",
    "    if not isinstance(x, list):\n",
    "        x = list(x)\n",
    "    # ensure strings\n",
    "    x = [\"\" if t is None else str(t) for t in x]\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_router_features(texts, bert_tok, bert_model, device, max_len=128, batch_size=64):\n",
    "    texts = safe_text_list(texts)\n",
    "    feats = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = bert_tok(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        logits = bert_model(**enc).logits  # [B, num_labels]\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "        conf = probs.max(axis=1)  # max prob\n",
    "        top2 = np.sort(probs, axis=1)[:, -2:]\n",
    "        margin = top2[:, 1] - top2[:, 0]\n",
    "        entropy = -(probs * np.log(probs + 1e-12)).sum(axis=1)\n",
    "\n",
    "        # token length of each text (approx, not padded)\n",
    "        lens = np.array([len(bert_tok(t)[\"input_ids\"]) for t in batch])\n",
    "\n",
    "        feats.append(np.column_stack([conf, margin, entropy, lens]))\n",
    "\n",
    "    return np.vstack(feats) if len(feats) else np.zeros((0, 4), dtype=float)\n",
    "\n",
    "def route_with_threshold(router, X, threshold):\n",
    "    probs = router.predict_proba(X)[:, 1]  # P(complex)\n",
    "    route = (probs > float(threshold)).astype(int)  # 1=complex,0=simple\n",
    "    return probs, route\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYjkqv2-kdI4",
    "outputId": "0b080688-aa9b-41bd-e814-27b2f5836e13"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_texts = safe_text_list(eval_i[\"instruction\"])\n",
    "X_eval = build_router_features(eval_texts, bert_tok, bert_model, device, max_len=128, batch_size=64)\n",
    "\n",
    "probs_eval, route_eval = route_with_threshold(router, X_eval, THRESHOLD)\n",
    "\n",
    "print(\"EVAL size:\", len(eval_texts))\n",
    "print(\"Threshold:\", THRESHOLD)\n",
    "print(\"Complex rate:\", round(route_eval.mean(), 3))\n",
    "\n",
    "print(\"\\nProbability summary (P(complex)):\")\n",
    "print(\" min:\", round(float(probs_eval.min()), 3),\n",
    "      \" p25:\", round(float(np.quantile(probs_eval, 0.25)), 3),\n",
    "      \" median:\", round(float(np.median(probs_eval)), 3),\n",
    "      \" p75:\", round(float(np.quantile(probs_eval, 0.75)), 3),\n",
    "      \" max:\", round(float(probs_eval.max()), 3))\n",
    "\n",
    "# sanity: are we in a non-degenerate regime?\n",
    "if route_eval.mean() < 0.05 or route_eval.mean() > 0.95:\n",
    "    print(\"\\n⚠️ WARNING: routing is near-degenerate (almost all one side). Consider re-tuning threshold.\")\n",
    "else:\n",
    "    print(\"\\n✅ Routing is non-degenerate (balanced enough).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7SW_L7yloJJ"
   },
   "source": [
    "לכל intent:\n",
    "\n",
    "כמה אחוז מהשאילתות נשלחות ל-complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQRI3Y-9kn4Y",
    "outputId": "f6154b22-6af7-4c1b-c0fd-79db845a9b01"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "eval_intents = safe_text_list(eval_i[\"intent_pred\"])\n",
    "\n",
    "# guard: lengths must match\n",
    "n = min(len(eval_intents), len(route_eval))\n",
    "eval_intents = eval_intents[:n]\n",
    "route_eval_ = route_eval[:n]\n",
    "\n",
    "counts = Counter(eval_intents)\n",
    "rates = defaultdict(list)\n",
    "\n",
    "for intent, r in zip(eval_intents, route_eval_):\n",
    "    rates[intent].append(int(r))\n",
    "\n",
    "# build a sorted table (top intents by frequency)\n",
    "rows = []\n",
    "for intent, c in counts.most_common(30):\n",
    "    cr = float(np.mean(rates[intent]))\n",
    "    rows.append((intent, c, cr))\n",
    "\n",
    "print(\"Top intents (by frequency) + Complex rate:\")\n",
    "for intent, c, cr in rows:\n",
    "    print(f\"{intent:30s} | n={c:4d} | complex_rate={cr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKCmel0lkslg",
    "outputId": "d3018001-17f2-4cc8-e030-875a85140840"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X_eval columns: [conf, margin, entropy, len]\n",
    "conf, margin, entropy, length = X_eval[:,0], X_eval[:,1], X_eval[:,2], X_eval[:,3]\n",
    "\n",
    "def summarize(name, arr, mask):\n",
    "    a = arr[mask]\n",
    "    if len(a) == 0:\n",
    "        print(name, \"EMPTY\")\n",
    "        return\n",
    "    print(f\"{name:10s} | mean={a.mean():.4f}  median={np.median(a):.4f}  p25={np.quantile(a,0.25):.4f}  p75={np.quantile(a,0.75):.4f}\")\n",
    "\n",
    "mask_c = (route_eval == 1)\n",
    "mask_s = (route_eval == 0)\n",
    "\n",
    "print(\"=== Feature stats by routed class ===\")\n",
    "print(\"\\n--- SIMPLE (routed) ---\")\n",
    "summarize(\"conf\", conf, mask_s)\n",
    "summarize(\"margin\", margin, mask_s)\n",
    "summarize(\"entropy\", entropy, mask_s)\n",
    "summarize(\"len\", length, mask_s)\n",
    "\n",
    "print(\"\\n--- COMPLEX (routed) ---\")\n",
    "summarize(\"conf\", conf, mask_c)\n",
    "summarize(\"margin\", margin, mask_c)\n",
    "summarize(\"entropy\", entropy, mask_c)\n",
    "summarize(\"len\", length, mask_c)\n",
    "\n",
    "print(\"\\nExpected trends (good sign):\")\n",
    "print(\"- complex: higher entropy, lower margin/conf, longer length (often).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwxZzYJAmA5R"
   },
   "source": [
    "מסקנה נכונה\n",
    "\n",
    "Routing is influenced by higher-level semantic signals beyond token-level uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXApaOnVkwAQ",
    "outputId": "abf1aded-94eb-4642-a325-a10c273b9fe6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# assumes you have X_test, y_test from router split\n",
    "probs_test = router.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0.3, 0.9, 61)\n",
    "curve = []\n",
    "\n",
    "for t in thresholds:\n",
    "    pred = (probs_test > t).astype(int)\n",
    "    rate = pred.mean()\n",
    "    # macro-F1 gives equal weight to both classes\n",
    "    f1m = f1_score(y_test, pred, average=\"macro\")\n",
    "    curve.append((t, rate, f1m))\n",
    "\n",
    "# print best by macro-F1 but keep non-degenerate routing\n",
    "curve_nd = [x for x in curve if 0.05 < x[1] < 0.95]\n",
    "best = max(curve_nd, key=lambda x: x[2])\n",
    "\n",
    "print(\"Best (non-degenerate) threshold by macro-F1:\")\n",
    "print(\"t =\", round(best[0], 3), \"| complex_rate =\", round(best[1], 3), \"| macroF1 =\", round(best[2], 3))\n",
    "\n",
    "print(\"\\nSample points:\")\n",
    "for t, rate, f1m in curve[::10]:\n",
    "    print(\"t=\", round(t,3), \" rate=\", round(rate,3), \" macroF1=\", round(f1m,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3w1jcwR6k0WY",
    "outputId": "631594a5-f489-47b2-e2a9-777c386e0613"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def perturb(text):\n",
    "    # very light perturbations\n",
    "    options = [\n",
    "        lambda s: s.strip(),\n",
    "        lambda s: \"please \" + s,\n",
    "        lambda s: s + \" thanks\",\n",
    "        lambda s: s.replace(\"?\", \"\").strip(),\n",
    "        lambda s: s.replace(\"  \", \" \")\n",
    "    ]\n",
    "    f = random.choice(options)\n",
    "    return f(text)\n",
    "\n",
    "sample_n = 200\n",
    "idxs = list(range(len(eval_texts)))\n",
    "random.shuffle(idxs)\n",
    "idxs = idxs[:min(sample_n, len(idxs))]\n",
    "\n",
    "orig = [eval_texts[i] for i in idxs]\n",
    "pert = [perturb(t) for t in orig]\n",
    "\n",
    "X_o = build_router_features(orig, bert_tok, bert_model, device, max_len=128, batch_size=64)\n",
    "X_p = build_router_features(pert, bert_tok, bert_model, device, max_len=128, batch_size=64)\n",
    "\n",
    "_, r_o = route_with_threshold(router, X_o, THRESHOLD)\n",
    "_, r_p = route_with_threshold(router, X_p, THRESHOLD)\n",
    "\n",
    "flip_rate = (r_o != r_p).mean()\n",
    "print(\"Robustness test on\", len(orig), \"samples\")\n",
    "print(\"Flip rate (routing changed):\", round(float(flip_rate), 3))\n",
    "\n",
    "# show a few flips\n",
    "flips = np.where(r_o != r_p)[0][:10]\n",
    "for j in flips:\n",
    "    print(\"\\n--- FLIP EXAMPLE ---\")\n",
    "    print(\"orig :\", orig[j])\n",
    "    print(\"pert :\", pert[j])\n",
    "    print(\"route orig:\", \"complex\" if r_o[j]==1 else \"simple\",\n",
    "          \"| route pert:\", \"complex\" if r_p[j]==1 else \"simple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVO1_IpsmvzH"
   },
   "source": [
    "איך לפרש את הגרף (למאמר)\n",
    "\n",
    "Macro-F1 מייצג איכות routing\n",
    "\n",
    "Complex Rate מייצג עלות חישובית\n",
    "\n",
    "הקו האנכי = threshold שנבחר\n",
    "\n",
    "רואים בבירור trade-off:\n",
    "\n",
    "threshold נמוך → איכות ↑ אבל עלות ↑\n",
    "\n",
    "threshold גבוה → עלות ↓ אבל איכות ↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "DX7Mw7TEmppq",
    "outputId": "b047f6b7-1799-43bb-bd5c-426824278544"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unpack curve\n",
    "ts = [x[0] for x in curve]\n",
    "complex_rates = [x[1] for x in curve]\n",
    "macro_f1s = [x[2] for x in curve]\n",
    "\n",
    "best_t, best_rate, best_f1 = best\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# macro-F1\n",
    "plt.plot(ts, macro_f1s, label=\"Macro-F1\", linewidth=2)\n",
    "\n",
    "# complex rate\n",
    "plt.plot(ts, complex_rates, label=\"Complex Rate\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# chosen threshold\n",
    "plt.axvline(best_t, linestyle=\":\", linewidth=2)\n",
    "plt.scatter([best_t], [best_f1], zorder=5)\n",
    "plt.scatter([best_t], [best_rate], zorder=5)\n",
    "\n",
    "plt.xlabel(\"Routing Threshold\")\n",
    "plt.ylabel(\"Score / Rate\")\n",
    "plt.title(\"Trade-off between Routing Quality and Computational Cost\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "dEokkn8WnNvA",
    "outputId": "59708d6b-17fc-40eb-d9d2-b9bba2ee2507"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# safety: ensure lists\n",
    "intents = list(eval_i[\"intent_pred\"])\n",
    "routes = route_eval.astype(int)\n",
    "\n",
    "# align lengths just in case\n",
    "n = min(len(intents), len(routes))\n",
    "intents = intents[:n]\n",
    "routes = routes[:n]\n",
    "\n",
    "# compute complex rate per intent\n",
    "counts = Counter(intents)\n",
    "complex_counts = defaultdict(int)\n",
    "\n",
    "for intent, r in zip(intents, routes):\n",
    "    complex_counts[intent] += r\n",
    "\n",
    "intent_rates = [\n",
    "    (intent, complex_counts[intent] / counts[intent])\n",
    "    for intent in counts\n",
    "]\n",
    "\n",
    "# sort by complex rate (descending)\n",
    "intent_rates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# take top-N for readability (recommended for paper)\n",
    "TOP_N = 20\n",
    "intent_rates = intent_rates[:TOP_N]\n",
    "\n",
    "labels = [x[0] for x in intent_rates]\n",
    "rates = [x[1] for x in intent_rates]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, rates)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Complex Routing Rate\")\n",
    "plt.xlabel(\"Intent\")\n",
    "plt.title(\"Complex Routing Rate per Intent (Top Intents)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 945
    },
    "id": "oTNw964lpOjQ",
    "outputId": "f565cfec-aafa-4c91-d67b-93399251d7ab"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = {\n",
    "    \"Entropy\": entropy,\n",
    "    \"Margin\": margin,\n",
    "    \"Confidence\": conf,\n",
    "    \"Length\": length\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, values) in zip(axes, features.items()):\n",
    "    ax.boxplot(\n",
    "        [values[route_eval == 0], values[route_eval == 1]],\n",
    "        labels=[\"Simple\", \"Complex\"],\n",
    "        showfliers=False\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"Feature Distributions by Routing Decision\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19305,
     "status": "ok",
     "timestamp": 1771152944897,
     "user": {
      "displayName": "Anaelle Zerbib",
      "userId": "08268450206902884295"
     },
     "user_tz": -120
    },
    "id": "kqop7ChF8NTs",
    "outputId": "ff40a79b-e2ad-46c8-8ecb-41b557ee2440"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
